{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5813f6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Results saved to output_with_toxicity.csv\n",
      "\n",
      "Summary:\n",
      "Total texts: 40\n",
      "Toxic: 2\n",
      "Non-Toxic: 38\n"
     ]
    }
   ],
   "source": [
    "# Ensure required packages are installed in the notebook environment\n",
    "%pip install google-api-python-client python-dotenv pandas -q\n",
    "\n",
    "# imports (placed at top as required)\n",
    "from googleapiclient import discovery\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not API_KEY:\n",
    "    raise EnvironmentError(\"GOOGLE_API_KEY not found. Set it in a .env file or in environment variables before running this cell.\")\n",
    "\n",
    "# creates connection to Google's Perspective API\n",
    "try:\n",
    "    client = discovery.build(\n",
    "        \"commentanalyzer\", \"v1alpha1\",\n",
    "        developerKey=API_KEY,\n",
    "        discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "        static_discovery=False,\n",
    "    )\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to create commentanalyzer client: {e}\")\n",
    "\n",
    "# read input CSV (ensure data.csv exists in the working directory)\n",
    "df = pd.read_csv('data.csv')\n",
    "texts = df.iloc[:, 0]\n",
    "toxicityScores = []\n",
    "\n",
    "for idx, text in enumerate(texts):\n",
    "    try:\n",
    "        analyze_request = {\n",
    "            'comment': {'text': str(text)},\n",
    "            'requestedAttributes': {'TOXICITY': {}}\n",
    "        }\n",
    "\n",
    "        response = client.comments().analyze(body=analyze_request).execute()\n",
    "        score = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "        toxicityScores.append(score)\n",
    "\n",
    "        time.sleep(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {idx}: {e}\")\n",
    "        toxicityScores.append(None)\n",
    "\n",
    "output_df = pd.DataFrame({\n",
    "    'text': texts,\n",
    "    'toxicity_score': toxicityScores,\n",
    "    'predicted_label': ['toxic' if score and score > 0.5 else 'non-toxic' for score in toxicityScores]\n",
    "})\n",
    "\n",
    "output_df.to_csv('output_with_toxicity.csv', index=False)\n",
    "print(\"\\nResults saved to output_with_toxicity.csv\")\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"Total texts: {len(output_df)}\")\n",
    "print(f\"Toxic: {(output_df['predicted_label'] == 'toxic').sum()}\")\n",
    "print(f\"Non-Toxic: {(output_df['predicted_label'] == 'non-toxic').sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a7d47b",
   "metadata": {},
   "source": [
    "# Protected Attribute and Creating Examples\n",
    "\n",
    "I will be choosing the protected attribute of Gender. I'll manually create 20 examples (10 toxic, 10 non-toxic) for each gender (Male and Female), resulting in 40 examples total. This will be stored in the CSV file data.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2fa07b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected argument value expression (3428037032.py, line 7)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mgenai.configure(api_key=)\u001b[39m\n                    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m expected argument value expression\n"
     ]
    }
   ],
   "source": [
    "# using gemini to generate toxicity judgement\n",
    "%pip install google-generativeai\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"GEMINI_API_KEY\")\n",
    "\n",
    "def get_gemini_toxicity(text):\n",
    "    prompt = f\"Rate the toxicity of this sentence from 0 (not toxic) to 1 (very toxic). Return only the score and nothing else: '{text}\"\n",
    "    response = genai.GenerativeModel(\"gemini-2.0-flash-lite\").generate_content(prompt)\n",
    "    print(\"Gemini response: \", response.text)\n",
    "    try:\n",
    "        # extract numeric score if possible\n",
    "        score = float(response.text.strip().split()[0])\n",
    "    except:\n",
    "        score = None\n",
    "    time.sleep(2)\n",
    "    return score\n",
    "\n",
    "get_gemini_toxicity()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
